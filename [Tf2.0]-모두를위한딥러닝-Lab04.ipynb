{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "x_data= [[72.,80.,75.], [93.,88.,93],[89.,91.,90.],[96.,98.,100.],\n",
    "        [73.,66.,70.]]\n",
    "y_data=[[152.],[185.],[180.],[196.],[142.]]\n",
    "\n",
    "W = tf.Variable(tf.random.normal([3,1]))\n",
    "b = tf.Variable(tf.random.normal([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  0\n",
      "cost:  0.1880208\n",
      "hypothesis: \n",
      " [[153.9906 ]\n",
      " [187.09277]\n",
      " [183.02654]\n",
      " [199.39499]\n",
      " [144.45526]]\n",
      "=======================\n",
      "step:  20\n",
      "cost:  0.46514517\n",
      "hypothesis: \n",
      " [[152.3121 ]\n",
      " [185.06825]\n",
      " [181.03145]\n",
      " [197.22322]\n",
      " [142.90926]]\n",
      "=======================\n",
      "step:  40\n",
      "cost:  0.22975786\n",
      "hypothesis: \n",
      " [[151.7629 ]\n",
      " [184.4058 ]\n",
      " [180.37863]\n",
      " [196.51262]\n",
      " [142.4034 ]]\n",
      "=======================\n",
      "step:  60\n",
      "cost:  0.19181056\n",
      "hypothesis: \n",
      " [[151.69759]\n",
      " [184.32703]\n",
      " [180.30098]\n",
      " [196.42813]\n",
      " [142.34325]]\n",
      "=======================\n",
      "step:  80\n",
      "cost:  0.1878592\n",
      "hypothesis: \n",
      " [[151.7089 ]\n",
      " [184.34065]\n",
      " [180.31438]\n",
      " [196.44276]\n",
      " [142.35367]]\n",
      "=======================\n",
      "step:  100\n",
      "cost:  0.18686499\n",
      "hypothesis: \n",
      " [[151.68048]\n",
      " [184.30637]\n",
      " [180.28056]\n",
      " [196.40599]\n",
      " [142.32748]]\n",
      "=======================\n",
      "step:  120\n",
      "cost:  0.18680541\n",
      "hypothesis: \n",
      " [[151.67798]\n",
      " [184.30334]\n",
      " [180.27756]\n",
      " [196.40277]\n",
      " [142.3252 ]]\n",
      "=======================\n",
      "step:  140\n",
      "cost:  0.18680112\n",
      "hypothesis: \n",
      " [[151.68002]\n",
      " [184.30579]\n",
      " [180.27994]\n",
      " [196.40541]\n",
      " [142.32707]]\n",
      "=======================\n",
      "step:  160\n",
      "cost:  0.186799\n",
      "hypothesis: \n",
      " [[151.68024]\n",
      " [184.30603]\n",
      " [180.28014]\n",
      " [196.40569]\n",
      " [142.32727]]\n",
      "=======================\n",
      "step:  180\n",
      "cost:  0.1868022\n",
      "hypothesis: \n",
      " [[151.67973]\n",
      " [184.3054 ]\n",
      " [180.27948]\n",
      " [196.40504]\n",
      " [142.3268 ]]\n",
      "=======================\n",
      "step:  200\n",
      "cost:  0.18680348\n",
      "hypothesis: \n",
      " [[151.6798 ]\n",
      " [184.30547]\n",
      " [180.27951]\n",
      " [196.40514]\n",
      " [142.32687]]\n",
      "=======================\n",
      "step:  220\n",
      "cost:  0.18680134\n",
      "hypothesis: \n",
      " [[151.67987]\n",
      " [184.30553]\n",
      " [180.27953]\n",
      " [196.40523]\n",
      " [142.32693]]\n",
      "=======================\n",
      "step:  240\n",
      "cost:  0.18679437\n",
      "hypothesis: \n",
      " [[151.6799 ]\n",
      " [184.30554]\n",
      " [180.27951]\n",
      " [196.40527]\n",
      " [142.32697]]\n",
      "=======================\n",
      "step:  260\n",
      "cost:  0.18679853\n",
      "hypothesis: \n",
      " [[151.67993]\n",
      " [184.30556]\n",
      " [180.27948]\n",
      " [196.40532]\n",
      " [142.327  ]]\n",
      "=======================\n",
      "step:  280\n",
      "cost:  0.18680069\n",
      "hypothesis: \n",
      " [[151.67992]\n",
      " [184.30553]\n",
      " [180.27939]\n",
      " [196.4053 ]\n",
      " [142.32698]]\n",
      "=======================\n",
      "step:  300\n",
      "cost:  0.18679887\n",
      "hypothesis: \n",
      " [[151.67996]\n",
      " [184.30554]\n",
      " [180.27936]\n",
      " [196.40535]\n",
      " [142.32701]]\n",
      "=======================\n",
      "step:  320\n",
      "cost:  0.18679255\n",
      "hypothesis: \n",
      " [[151.68   ]\n",
      " [184.30556]\n",
      " [180.27931]\n",
      " [196.40538]\n",
      " [142.32704]]\n",
      "=======================\n",
      "step:  340\n",
      "cost:  0.1867922\n",
      "hypothesis: \n",
      " [[151.68002]\n",
      " [184.30556]\n",
      " [180.27927]\n",
      " [196.40543]\n",
      " [142.32706]]\n",
      "=======================\n",
      "step:  360\n",
      "cost:  0.18679067\n",
      "hypothesis: \n",
      " [[151.68002]\n",
      " [184.30554]\n",
      " [180.27919]\n",
      " [196.40543]\n",
      " [142.32707]]\n",
      "=======================\n",
      "step:  380\n",
      "cost:  0.18679307\n",
      "hypothesis: \n",
      " [[151.68005]\n",
      " [184.30553]\n",
      " [180.27913]\n",
      " [196.40546]\n",
      " [142.32709]]\n",
      "=======================\n",
      "step:  400\n",
      "cost:  0.1867893\n",
      "hypothesis: \n",
      " [[151.68007]\n",
      " [184.30553]\n",
      " [180.27907]\n",
      " [196.40549]\n",
      " [142.3271 ]]\n",
      "=======================\n",
      "step:  420\n",
      "cost:  0.18678923\n",
      "hypothesis: \n",
      " [[151.68008]\n",
      " [184.30551]\n",
      " [180.27899]\n",
      " [196.40552]\n",
      " [142.32712]]\n",
      "=======================\n",
      "step:  440\n",
      "cost:  0.18678996\n",
      "hypothesis: \n",
      " [[151.6801 ]\n",
      " [184.30547]\n",
      " [180.2789 ]\n",
      " [196.40552]\n",
      " [142.3271 ]]\n",
      "=======================\n",
      "step:  460\n",
      "cost:  0.18678398\n",
      "hypothesis: \n",
      " [[151.68015]\n",
      " [184.3055 ]\n",
      " [180.27887]\n",
      " [196.4056 ]\n",
      " [142.32715]]\n",
      "=======================\n",
      "step:  480\n",
      "cost:  0.18678693\n",
      "hypothesis: \n",
      " [[151.68015]\n",
      " [184.30547]\n",
      " [180.27878]\n",
      " [196.4056 ]\n",
      " [142.32716]]\n",
      "=======================\n",
      "step:  500\n",
      "cost:  0.1867859\n",
      "hypothesis: \n",
      " [[151.68018]\n",
      " [184.30548]\n",
      " [180.27872]\n",
      " [196.40564]\n",
      " [142.32718]]\n",
      "=======================\n",
      "step:  520\n",
      "cost:  0.18678461\n",
      "hypothesis: \n",
      " [[151.68024]\n",
      " [184.3055 ]\n",
      " [180.27867]\n",
      " [196.4057 ]\n",
      " [142.32722]]\n",
      "=======================\n",
      "step:  540\n",
      "cost:  0.18677898\n",
      "hypothesis: \n",
      " [[151.68025]\n",
      " [184.30548]\n",
      " [180.27858]\n",
      " [196.40573]\n",
      " [142.32724]]\n",
      "=======================\n",
      "step:  560\n",
      "cost:  0.18677774\n",
      "hypothesis: \n",
      " [[151.68028]\n",
      " [184.30548]\n",
      " [180.27852]\n",
      " [196.40576]\n",
      " [142.32727]]\n",
      "=======================\n",
      "step:  580\n",
      "cost:  0.18677483\n",
      "hypothesis: \n",
      " [[151.68031]\n",
      " [184.30547]\n",
      " [180.27843]\n",
      " [196.4058 ]\n",
      " [142.32729]]\n",
      "=======================\n",
      "step:  600\n",
      "cost:  0.18677804\n",
      "hypothesis: \n",
      " [[151.6803 ]\n",
      " [184.30542]\n",
      " [180.27832]\n",
      " [196.40579]\n",
      " [142.32727]]\n",
      "=======================\n",
      "step:  620\n",
      "cost:  0.18677457\n",
      "hypothesis: \n",
      " [[151.68036]\n",
      " [184.30545]\n",
      " [180.27826]\n",
      " [196.40588]\n",
      " [142.32733]]\n",
      "=======================\n",
      "step:  640\n",
      "cost:  0.18677363\n",
      "hypothesis: \n",
      " [[151.68037]\n",
      " [184.30544]\n",
      " [180.27817]\n",
      " [196.4059 ]\n",
      " [142.32735]]\n",
      "=======================\n",
      "step:  660\n",
      "cost:  0.18677512\n",
      "hypothesis: \n",
      " [[151.68044]\n",
      " [184.30547]\n",
      " [180.27812]\n",
      " [196.40598]\n",
      " [142.3274 ]]\n",
      "=======================\n",
      "step:  680\n",
      "cost:  0.18677214\n",
      "hypothesis: \n",
      " [[151.68044]\n",
      " [184.30544]\n",
      " [180.278  ]\n",
      " [196.40599]\n",
      " [142.32741]]\n",
      "=======================\n",
      "step:  700\n",
      "cost:  0.18676595\n",
      "hypothesis: \n",
      " [[151.68051]\n",
      " [184.30545]\n",
      " [180.27795]\n",
      " [196.40607]\n",
      " [142.32744]]\n",
      "=======================\n",
      "step:  720\n",
      "cost:  0.1867692\n",
      "hypothesis: \n",
      " [[151.68051]\n",
      " [184.30544]\n",
      " [180.27785]\n",
      " [196.4061 ]\n",
      " [142.32745]]\n",
      "=======================\n",
      "step:  740\n",
      "cost:  0.18676393\n",
      "hypothesis: \n",
      " [[151.68059]\n",
      " [184.30544]\n",
      " [180.27779]\n",
      " [196.40616]\n",
      " [142.3275 ]]\n",
      "=======================\n",
      "step:  760\n",
      "cost:  0.18676172\n",
      "hypothesis: \n",
      " [[151.68057]\n",
      " [184.30542]\n",
      " [180.27765]\n",
      " [196.40617]\n",
      " [142.32751]]\n",
      "=======================\n",
      "step:  780\n",
      "cost:  0.18676339\n",
      "hypothesis: \n",
      " [[151.68063]\n",
      " [184.30542]\n",
      " [180.27759]\n",
      " [196.40625]\n",
      " [142.32755]]\n",
      "=======================\n",
      "step:  800\n",
      "cost:  0.1867608\n",
      "hypothesis: \n",
      " [[151.68065]\n",
      " [184.3054 ]\n",
      " [180.27747]\n",
      " [196.40627]\n",
      " [142.32756]]\n",
      "=======================\n",
      "step:  820\n",
      "cost:  0.18675245\n",
      "hypothesis: \n",
      " [[151.68068]\n",
      " [184.30539]\n",
      " [180.27737]\n",
      " [196.40631]\n",
      " [142.32758]]\n",
      "=======================\n",
      "step:  840\n",
      "cost:  0.18675417\n",
      "hypothesis: \n",
      " [[151.68074]\n",
      " [184.30539]\n",
      " [180.2773 ]\n",
      " [196.40636]\n",
      " [142.32762]]\n",
      "=======================\n",
      "step:  860\n",
      "cost:  0.18675412\n",
      "hypothesis: \n",
      " [[151.68079]\n",
      " [184.30539]\n",
      " [180.27722]\n",
      " [196.40643]\n",
      " [142.32767]]\n",
      "=======================\n",
      "step:  880\n",
      "cost:  0.18674973\n",
      "hypothesis: \n",
      " [[151.68077]\n",
      " [184.30534]\n",
      " [180.27705]\n",
      " [196.40645]\n",
      " [142.32767]]\n",
      "=======================\n",
      "step:  900\n",
      "cost:  0.18674892\n",
      "hypothesis: \n",
      " [[151.68085]\n",
      " [184.30536]\n",
      " [180.27698]\n",
      " [196.4065 ]\n",
      " [142.3277 ]]\n",
      "=======================\n",
      "step:  920\n",
      "cost:  0.1867474\n",
      "hypothesis: \n",
      " [[151.68088]\n",
      " [184.30536]\n",
      " [180.27689]\n",
      " [196.40659]\n",
      " [142.32776]]\n",
      "=======================\n",
      "step:  940\n",
      "cost:  0.18674563\n",
      "hypothesis: \n",
      " [[151.6809 ]\n",
      " [184.30533]\n",
      " [180.27676]\n",
      " [196.40659]\n",
      " [142.32776]]\n",
      "=======================\n",
      "step:  960\n",
      "cost:  0.18673891\n",
      "hypothesis: \n",
      " [[151.68094]\n",
      " [184.30533]\n",
      " [180.27666]\n",
      " [196.40665]\n",
      " [142.32779]]\n",
      "=======================\n",
      "step:  980\n",
      "cost:  0.18674143\n",
      "hypothesis: \n",
      " [[151.681  ]\n",
      " [184.30533]\n",
      " [180.27655]\n",
      " [196.40671]\n",
      " [142.32784]]\n",
      "=======================\n",
      "step:  1000\n",
      "cost:  0.1867393\n",
      "hypothesis: \n",
      " [[151.68097]\n",
      " [184.30525]\n",
      " [180.27638]\n",
      " [196.4067 ]\n",
      " [142.32782]]\n",
      "=======================\n",
      "step:  1020\n",
      "cost:  0.18674196\n",
      "hypothesis: \n",
      " [[151.68109]\n",
      " [184.30533]\n",
      " [180.27637]\n",
      " [196.40683]\n",
      " [142.32791]]\n",
      "=======================\n",
      "step:  1040\n",
      "cost:  0.18673389\n",
      "hypothesis: \n",
      " [[151.68105]\n",
      " [184.30524]\n",
      " [180.27617]\n",
      " [196.40678]\n",
      " [142.32788]]\n",
      "=======================\n",
      "step:  1060\n",
      "cost:  0.1867299\n",
      "hypothesis: \n",
      " [[151.68118]\n",
      " [184.30533]\n",
      " [180.27615]\n",
      " [196.40695]\n",
      " [142.328  ]]\n",
      "=======================\n",
      "step:  1080\n",
      "cost:  0.18673034\n",
      "hypothesis: \n",
      " [[151.68118]\n",
      " [184.30528]\n",
      " [180.276  ]\n",
      " [196.40697]\n",
      " [142.32799]]\n",
      "=======================\n",
      "step:  1100\n",
      "cost:  0.18672656\n",
      "hypothesis: \n",
      " [[151.68121]\n",
      " [184.30527]\n",
      " [180.27588]\n",
      " [196.40701]\n",
      " [142.328  ]]\n",
      "=======================\n",
      "step:  1120\n",
      "cost:  0.18673003\n",
      "hypothesis: \n",
      " [[151.68127]\n",
      " [184.30527]\n",
      " [180.27579]\n",
      " [196.4071 ]\n",
      " [142.32806]]\n",
      "=======================\n",
      "step:  1140\n",
      "cost:  0.18672207\n",
      "hypothesis: \n",
      " [[151.6813 ]\n",
      " [184.30525]\n",
      " [180.27565]\n",
      " [196.40714]\n",
      " [142.3281 ]]\n",
      "=======================\n",
      "step:  1160\n",
      "cost:  0.18672311\n",
      "hypothesis: \n",
      " [[151.68138]\n",
      " [184.30527]\n",
      " [180.27556]\n",
      " [196.40723]\n",
      " [142.32814]]\n",
      "=======================\n",
      "step:  1180\n",
      "cost:  0.18672106\n",
      "hypothesis: \n",
      " [[151.68143]\n",
      " [184.30527]\n",
      " [180.27542]\n",
      " [196.40729]\n",
      " [142.32817]]\n",
      "=======================\n",
      "step:  1200\n",
      "cost:  0.18672028\n",
      "hypothesis: \n",
      " [[151.68144]\n",
      " [184.30522]\n",
      " [180.27528]\n",
      " [196.4073 ]\n",
      " [142.32819]]\n",
      "=======================\n",
      "step:  1220\n",
      "cost:  0.1867229\n",
      "hypothesis: \n",
      " [[151.6815 ]\n",
      " [184.30522]\n",
      " [180.27519]\n",
      " [196.4074 ]\n",
      " [142.32823]]\n",
      "=======================\n",
      "step:  1240\n",
      "cost:  0.1867139\n",
      "hypothesis: \n",
      " [[151.68152]\n",
      " [184.30519]\n",
      " [180.27504]\n",
      " [196.40742]\n",
      " [142.32826]]\n",
      "=======================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  1260\n",
      "cost:  0.18671677\n",
      "hypothesis: \n",
      " [[151.68156]\n",
      " [184.30516]\n",
      " [180.27489]\n",
      " [196.40747]\n",
      " [142.32828]]\n",
      "=======================\n",
      "step:  1280\n",
      "cost:  0.18670784\n",
      "hypothesis: \n",
      " [[151.68164]\n",
      " [184.30519]\n",
      " [180.27483]\n",
      " [196.40758]\n",
      " [142.32835]]\n",
      "=======================\n",
      "step:  1300\n",
      "cost:  0.18670884\n",
      "hypothesis: \n",
      " [[151.68169]\n",
      " [184.3052 ]\n",
      " [180.2747 ]\n",
      " [196.40765]\n",
      " [142.3284 ]]\n",
      "=======================\n",
      "step:  1320\n",
      "cost:  0.18670788\n",
      "hypothesis: \n",
      " [[151.68176]\n",
      " [184.3052 ]\n",
      " [180.27458]\n",
      " [196.40773]\n",
      " [142.32845]]\n",
      "=======================\n",
      "step:  1340\n",
      "cost:  0.18670699\n",
      "hypothesis: \n",
      " [[151.68178]\n",
      " [184.30516]\n",
      " [180.27443]\n",
      " [196.40776]\n",
      " [142.32846]]\n",
      "=======================\n",
      "step:  1360\n",
      "cost:  0.1867023\n",
      "hypothesis: \n",
      " [[151.68182]\n",
      " [184.30518]\n",
      " [180.27429]\n",
      " [196.40784]\n",
      " [142.32852]]\n",
      "=======================\n",
      "step:  1380\n",
      "cost:  0.18670198\n",
      "hypothesis: \n",
      " [[151.68184]\n",
      " [184.30511]\n",
      " [180.27412]\n",
      " [196.40787]\n",
      " [142.32852]]\n",
      "=======================\n",
      "step:  1400\n",
      "cost:  0.18669471\n",
      "hypothesis: \n",
      " [[151.68185]\n",
      " [184.30505]\n",
      " [180.27394]\n",
      " [196.40787]\n",
      " [142.32852]]\n",
      "=======================\n",
      "step:  1420\n",
      "cost:  0.18670048\n",
      "hypothesis: \n",
      " [[151.68208]\n",
      " [184.30527]\n",
      " [180.27403]\n",
      " [196.40819]\n",
      " [142.32872]]\n",
      "=======================\n",
      "step:  1440\n",
      "cost:  0.18669139\n",
      "hypothesis: \n",
      " [[151.68199]\n",
      " [184.30508]\n",
      " [180.27371]\n",
      " [196.40805]\n",
      " [142.32863]]\n",
      "=======================\n",
      "step:  1460\n",
      "cost:  0.18669137\n",
      "hypothesis: \n",
      " [[151.682  ]\n",
      " [184.30505]\n",
      " [180.27354]\n",
      " [196.40808]\n",
      " [142.32864]]\n",
      "=======================\n",
      "step:  1480\n",
      "cost:  0.18668826\n",
      "hypothesis: \n",
      " [[151.68213]\n",
      " [184.30511]\n",
      " [180.2735 ]\n",
      " [196.40823]\n",
      " [142.32875]]\n",
      "=======================\n",
      "step:  1500\n",
      "cost:  0.18668397\n",
      "hypothesis: \n",
      " [[151.68224]\n",
      " [184.30518]\n",
      " [180.2734 ]\n",
      " [196.40836]\n",
      " [142.32884]]\n",
      "=======================\n",
      "step:  1520\n",
      "cost:  0.1866839\n",
      "hypothesis: \n",
      " [[151.68225]\n",
      " [184.30511]\n",
      " [180.27322]\n",
      " [196.40839]\n",
      " [142.32884]]\n",
      "=======================\n",
      "step:  1540\n",
      "cost:  0.18668158\n",
      "hypothesis: \n",
      " [[151.68231]\n",
      " [184.30511]\n",
      " [180.2731 ]\n",
      " [196.40848]\n",
      " [142.32889]]\n",
      "=======================\n",
      "step:  1560\n",
      "cost:  0.18667525\n",
      "hypothesis: \n",
      " [[151.68231]\n",
      " [184.30505]\n",
      " [180.27292]\n",
      " [196.4085 ]\n",
      " [142.3289 ]]\n",
      "=======================\n",
      "step:  1580\n",
      "cost:  0.18667924\n",
      "hypothesis: \n",
      " [[151.68237]\n",
      " [184.30505]\n",
      " [180.27277]\n",
      " [196.40857]\n",
      " [142.32895]]\n",
      "=======================\n",
      "step:  1600\n",
      "cost:  0.18667321\n",
      "hypothesis: \n",
      " [[151.68242]\n",
      " [184.30504]\n",
      " [180.27263]\n",
      " [196.40863]\n",
      " [142.32898]]\n",
      "=======================\n",
      "step:  1620\n",
      "cost:  0.18666674\n",
      "hypothesis: \n",
      " [[151.68251]\n",
      " [184.30505]\n",
      " [180.27252]\n",
      " [196.40875]\n",
      " [142.32907]]\n",
      "=======================\n",
      "step:  1640\n",
      "cost:  0.18667138\n",
      "hypothesis: \n",
      " [[151.68242]\n",
      " [184.30487]\n",
      " [180.2722 ]\n",
      " [196.40863]\n",
      " [142.32896]]\n",
      "=======================\n",
      "step:  1660\n",
      "cost:  0.18666516\n",
      "hypothesis: \n",
      " [[151.68262]\n",
      " [184.30505]\n",
      " [180.27223]\n",
      " [196.40889]\n",
      " [142.32916]]\n",
      "=======================\n",
      "step:  1680\n",
      "cost:  0.18666537\n",
      "hypothesis: \n",
      " [[151.68259]\n",
      " [184.30492]\n",
      " [180.27197]\n",
      " [196.40884]\n",
      " [142.3291 ]]\n",
      "=======================\n",
      "step:  1700\n",
      "cost:  0.18666545\n",
      "hypothesis: \n",
      " [[151.68277]\n",
      " [184.30505]\n",
      " [180.27197]\n",
      " [196.40907]\n",
      " [142.32925]]\n",
      "=======================\n",
      "step:  1720\n",
      "cost:  0.1866639\n",
      "hypothesis: \n",
      " [[151.68271]\n",
      " [184.3049 ]\n",
      " [180.27168]\n",
      " [196.40901]\n",
      " [142.32921]]\n",
      "=======================\n",
      "step:  1740\n",
      "cost:  0.18665558\n",
      "hypothesis: \n",
      " [[151.6828 ]\n",
      " [184.30496]\n",
      " [180.27158]\n",
      " [196.40913]\n",
      " [142.32928]]\n",
      "=======================\n",
      "step:  1760\n",
      "cost:  0.1866565\n",
      "hypothesis: \n",
      " [[151.68277]\n",
      " [184.30484]\n",
      " [180.27133]\n",
      " [196.4091 ]\n",
      " [142.32925]]\n",
      "=======================\n",
      "step:  1780\n",
      "cost:  0.18665251\n",
      "hypothesis: \n",
      " [[151.6829 ]\n",
      " [184.3049 ]\n",
      " [180.27127]\n",
      " [196.40927]\n",
      " [142.32938]]\n",
      "=======================\n",
      "step:  1800\n",
      "cost:  0.18664864\n",
      "hypothesis: \n",
      " [[151.68297]\n",
      " [184.30492]\n",
      " [180.2711 ]\n",
      " [196.40935]\n",
      " [142.32942]]\n",
      "=======================\n",
      "step:  1820\n",
      "cost:  0.18664697\n",
      "hypothesis: \n",
      " [[151.68312]\n",
      " [184.30502]\n",
      " [180.27109]\n",
      " [196.40956]\n",
      " [142.32956]]\n",
      "=======================\n",
      "step:  1840\n",
      "cost:  0.18664825\n",
      "hypothesis: \n",
      " [[151.68304]\n",
      " [184.30484]\n",
      " [180.27077]\n",
      " [196.40947]\n",
      " [142.32948]]\n",
      "=======================\n",
      "step:  1860\n",
      "cost:  0.18664455\n",
      "hypothesis: \n",
      " [[151.68318]\n",
      " [184.30493]\n",
      " [180.27069]\n",
      " [196.40964]\n",
      " [142.3296 ]]\n",
      "=======================\n",
      "step:  1880\n",
      "cost:  0.18663926\n",
      "hypothesis: \n",
      " [[151.6831 ]\n",
      " [184.30476]\n",
      " [180.27039]\n",
      " [196.40956]\n",
      " [142.32953]]\n",
      "=======================\n",
      "step:  1900\n",
      "cost:  0.18663988\n",
      "hypothesis: \n",
      " [[151.68323]\n",
      " [184.30482]\n",
      " [180.2703 ]\n",
      " [196.40971]\n",
      " [142.32962]]\n",
      "=======================\n",
      "step:  1920\n",
      "cost:  0.18663643\n",
      "hypothesis: \n",
      " [[151.68323]\n",
      " [184.30473]\n",
      " [180.27007]\n",
      " [196.4097 ]\n",
      " [142.32962]]\n",
      "=======================\n",
      "step:  1940\n",
      "cost:  0.18663712\n",
      "hypothesis: \n",
      " [[151.6835 ]\n",
      " [184.30498]\n",
      " [180.27016]\n",
      " [196.41008]\n",
      " [142.32988]]\n",
      "=======================\n",
      "step:  1960\n",
      "cost:  0.18663554\n",
      "hypothesis: \n",
      " [[151.68237]\n",
      " [184.30354]\n",
      " [180.26859]\n",
      " [196.40862]\n",
      " [142.32883]]\n",
      "=======================\n",
      "step:  1980\n",
      "cost:  0.1957241\n",
      "hypothesis: \n",
      " [[151.61726]\n",
      " [184.22493]\n",
      " [180.19096]\n",
      " [196.32437]\n",
      " [142.26884]]\n",
      "=======================\n",
      "step:  2000\n",
      "cost:  0.21456926\n",
      "hypothesis: \n",
      " [[151.57104]\n",
      " [184.16911]\n",
      " [180.13582]\n",
      " [196.26459]\n",
      " [142.22627]]\n",
      "=======================\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def cost():\n",
    "    hypothesis = tf.matmul(x_data, W) + b\n",
    "    cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
    "    return cost\n",
    "\n",
    "opt = tf.optimizers.Adam(lr=0.01)\n",
    "for step in range(2001):\n",
    "    hypo = tf.matmul(x_data, W) + b\n",
    "    opt.minimize(cost, var_list=[W,b])\n",
    "    if step % 20 == 0:\n",
    "        print(\"step: \",step)\n",
    "        print(\"cost: \",cost().numpy())\n",
    "        print(\"hypothesis: \\n\",hypo.numpy())\n",
    "        print(\"=======================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'DeepLearningZeroToAll'에 복제합니다...\n",
      "remote: Enumerating objects: 10, done.\u001b[K\n",
      "remote: Counting objects: 100% (10/10), done.\u001b[K\n",
      "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
      "remote: Total 1688 (delta 2), reused 6 (delta 2), pack-reused 1678\u001b[K\n",
      "\u001b[K오브젝트를 받는 중: 100% (1688/1688), 733.45 KiB | 937.00 KiB/s, 완료.\n",
      "\u001b[K델타를 알아내는 중: 100% (1101/1101), 완료.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/hunkim/DeepLearningZeroToAll.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'DeepLearningZeroToAll'\n",
      "/Users/yeonsookim/Desktop/code/DeepLearningZeroToAll\n"
     ]
    }
   ],
   "source": [
    "cd DeepLearningZeroToAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTRIBUTING.md\r\n",
      "README.md\r\n",
      "\u001b[1m\u001b[36mchainer\u001b[m\u001b[m/\r\n",
      "data-01-test-score.csv\r\n",
      "data-02-stock_daily.csv\r\n",
      "data-03-diabetes.csv\r\n",
      "data-04-zoo.csv\r\n",
      "\u001b[1m\u001b[36mipynb\u001b[m\u001b[m/\r\n",
      "\u001b[1m\u001b[36mkeras\u001b[m\u001b[m/\r\n",
      "lab-01-basics.ipynb\r\n",
      "lab-02-1-linear_regression.py\r\n",
      "lab-02-2-linear_regression_feed.py\r\n",
      "lab-02-3-linear_regression_tensorflow.org.py\r\n",
      "lab-03-1-minimizing_cost_show_graph.py\r\n",
      "lab-03-2-minimizing_cost_gradient_update.py\r\n",
      "lab-03-3-minimizing_cost_tf_optimizer.py\r\n",
      "lab-03-X-minimizing_cost_tf_gradient.py\r\n",
      "lab-04-1-multi_variable_linear_regression.py\r\n",
      "lab-04-2-multi_variable_matmul_linear_regression.py\r\n",
      "lab-04-3-file_input_linear_regression.py\r\n",
      "lab-04-4-tf_reader_linear_regression.py\r\n",
      "lab-05-1-logistic_regression.py\r\n",
      "lab-05-2-logistic_regression_diabetes.py\r\n",
      "lab-06-1-softmax_classifier.py\r\n",
      "lab-06-2-softmax_zoo_classifier.py\r\n",
      "lab-07-1-learning_rate_and_evaluation.py\r\n",
      "lab-07-2-linear_regression_without_min_max.py\r\n",
      "lab-07-3-linear_regression_min_max.py\r\n",
      "lab-07-4-mnist_introduction.py\r\n",
      "lab-08-tensor_manipulation.ipynb\r\n",
      "lab-09-1-xor.py\r\n",
      "lab-09-2-xor-nn.py\r\n",
      "lab-09-3-xor-nn-wide-deep.py\r\n",
      "lab-09-4-xor_tensorboard.py\r\n",
      "lab-09-5-linear_back_prop.py\r\n",
      "lab-09-6-multi-linear_back_prop.py\r\n",
      "lab-09-7-sigmoid_back_prop.py\r\n",
      "\u001b[31mlab-09-x-xor-nn-back_prop.py\u001b[m\u001b[m*\r\n",
      "lab-10-1-mnist_softmax.py\r\n",
      "lab-10-2-mnist_nn.py\r\n",
      "lab-10-3-mnist_nn_xavier.py\r\n",
      "lab-10-4-mnist_nn_deep.py\r\n",
      "lab-10-5-mnist_nn_dropout.py\r\n",
      "lab-10-6-mnist_nn_batchnorm.ipynb\r\n",
      "lab-10-7-mnist_nn_higher_level_API.py\r\n",
      "lab-10-8-mnist_nn_selu(wip).py\r\n",
      "lab-10-X1-mnist_back_prop.py\r\n",
      "lab-11-0-cnn_basics.ipynb\r\n",
      "lab-11-1-mnist_cnn.py\r\n",
      "lab-11-2-mnist_deep_cnn.py\r\n",
      "lab-11-3-mnist_cnn_class.py\r\n",
      "lab-11-4-mnist_cnn_layers.py\r\n",
      "lab-11-5-mnist_cnn_ensemble_layers.py\r\n",
      "lab-11-X-mnist_cnn_low_memory.py\r\n",
      "lab-12-0-rnn_basics.ipynb\r\n",
      "lab-12-1-hello-rnn.py\r\n",
      "lab-12-2-char-seq-rnn.py\r\n",
      "lab-12-3-char-seq-softmax-only.py\r\n",
      "lab-12-4-rnn_long_char.py\r\n",
      "lab-12-5-rnn_stock_prediction.py\r\n",
      "lab-13-1-mnist_using_scope.py\r\n",
      "lab-13-2-mnist_tensorboard.py\r\n",
      "lab-13-3-mnist_save_restore.py\r\n",
      "\u001b[1m\u001b[36mmxnet\u001b[m\u001b[m/\r\n",
      "\u001b[1m\u001b[36mnumpy\u001b[m\u001b[m/\r\n",
      "\u001b[1m\u001b[36mpytorch\u001b[m\u001b[m/\r\n",
      "requirements.txt\r\n",
      "\u001b[1m\u001b[36mtests\u001b[m\u001b[m/\r\n",
      "\u001b[1m\u001b[36mtf2\u001b[m\u001b[m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 3) [[ 73.  80.  75.]\n",
      " [ 93.  88.  93.]\n",
      " [ 89.  91.  90.]\n",
      " [ 96.  98. 100.]\n",
      " [ 73.  66.  70.]\n",
      " [ 53.  46.  55.]\n",
      " [ 69.  74.  77.]\n",
      " [ 47.  56.  60.]\n",
      " [ 87.  79.  90.]\n",
      " [ 79.  70.  88.]\n",
      " [ 69.  70.  73.]\n",
      " [ 70.  65.  74.]\n",
      " [ 93.  95.  91.]\n",
      " [ 79.  80.  73.]\n",
      " [ 70.  73.  78.]\n",
      " [ 93.  89.  96.]\n",
      " [ 78.  75.  68.]\n",
      " [ 81.  90.  93.]\n",
      " [ 88.  92.  86.]\n",
      " [ 78.  83.  77.]\n",
      " [ 82.  86.  90.]\n",
      " [ 86.  82.  89.]\n",
      " [ 78.  83.  85.]\n",
      " [ 76.  83.  71.]\n",
      " [ 96.  93.  95.]] 25\n",
      "(25, 1) [[152.]\n",
      " [185.]\n",
      " [180.]\n",
      " [196.]\n",
      " [142.]\n",
      " [101.]\n",
      " [149.]\n",
      " [115.]\n",
      " [175.]\n",
      " [164.]\n",
      " [141.]\n",
      " [141.]\n",
      " [184.]\n",
      " [152.]\n",
      " [148.]\n",
      " [192.]\n",
      " [147.]\n",
      " [183.]\n",
      " [177.]\n",
      " [159.]\n",
      " [177.]\n",
      " [175.]\n",
      " [175.]\n",
      " [149.]\n",
      " [192.]] 25\n"
     ]
    }
   ],
   "source": [
    "xy = np.loadtxt('data-01-test-score.csv', delimiter=',', dtype=np.float32)\n",
    "x_data = xy[:,0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "print(x_data.shape, x_data, len(x_data))\n",
    "print(y_data.shape, y_data, len(y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  0\n",
      "cost:  35003.047\n",
      "=======================\n",
      "step:  20\n",
      "cost:  2602.3784\n",
      "=======================\n",
      "step:  40\n",
      "cost:  212.03658\n",
      "=======================\n",
      "step:  60\n",
      "cost:  20.163202\n",
      "=======================\n",
      "step:  80\n",
      "cost:  26.019653\n",
      "=======================\n",
      "step:  100\n",
      "cost:  19.536186\n",
      "=======================\n",
      "step:  120\n",
      "cost:  19.295763\n",
      "=======================\n",
      "step:  140\n",
      "cost:  18.931965\n",
      "=======================\n",
      "step:  160\n",
      "cost:  18.663528\n",
      "=======================\n",
      "step:  180\n",
      "cost:  18.382473\n",
      "=======================\n",
      "step:  200\n",
      "cost:  18.091553\n",
      "=======================\n",
      "step:  220\n",
      "cost:  17.792324\n",
      "=======================\n",
      "step:  240\n",
      "cost:  17.486176\n",
      "=======================\n",
      "step:  260\n",
      "cost:  17.174501\n",
      "=======================\n",
      "step:  280\n",
      "cost:  16.858604\n",
      "=======================\n",
      "step:  300\n",
      "cost:  16.539701\n",
      "=======================\n",
      "step:  320\n",
      "cost:  16.218884\n",
      "=======================\n",
      "step:  340\n",
      "cost:  15.897186\n",
      "=======================\n",
      "step:  360\n",
      "cost:  15.575586\n",
      "=======================\n",
      "step:  380\n",
      "cost:  15.254902\n",
      "=======================\n",
      "step:  400\n",
      "cost:  14.935977\n",
      "=======================\n",
      "step:  420\n",
      "cost:  14.619523\n",
      "=======================\n",
      "step:  440\n",
      "cost:  14.306216\n",
      "=======================\n",
      "step:  460\n",
      "cost:  13.996693\n",
      "=======================\n",
      "step:  480\n",
      "cost:  13.691475\n",
      "=======================\n",
      "step:  500\n",
      "cost:  13.391086\n",
      "=======================\n",
      "step:  520\n",
      "cost:  13.095975\n",
      "=======================\n",
      "step:  540\n",
      "cost:  12.806516\n",
      "=======================\n",
      "step:  560\n",
      "cost:  12.5230665\n",
      "=======================\n",
      "step:  580\n",
      "cost:  12.245951\n",
      "=======================\n",
      "step:  600\n",
      "cost:  11.975424\n",
      "=======================\n",
      "step:  620\n",
      "cost:  11.711719\n",
      "=======================\n",
      "step:  640\n",
      "cost:  11.454992\n",
      "=======================\n",
      "step:  660\n",
      "cost:  11.205437\n",
      "=======================\n",
      "step:  680\n",
      "cost:  10.963143\n",
      "=======================\n",
      "step:  700\n",
      "cost:  10.728203\n",
      "=======================\n",
      "step:  720\n",
      "cost:  10.500689\n",
      "=======================\n",
      "step:  740\n",
      "cost:  10.280603\n",
      "=======================\n",
      "step:  760\n",
      "cost:  10.0679655\n",
      "=======================\n",
      "step:  780\n",
      "cost:  9.862766\n",
      "=======================\n",
      "step:  800\n",
      "cost:  9.664967\n",
      "=======================\n",
      "step:  820\n",
      "cost:  9.474491\n",
      "=======================\n",
      "step:  840\n",
      "cost:  9.291264\n",
      "=======================\n",
      "step:  860\n",
      "cost:  9.115227\n",
      "=======================\n",
      "step:  880\n",
      "cost:  8.946214\n",
      "=======================\n",
      "step:  900\n",
      "cost:  8.784173\n",
      "=======================\n",
      "step:  920\n",
      "cost:  8.628909\n",
      "=======================\n",
      "step:  940\n",
      "cost:  8.480335\n",
      "=======================\n",
      "step:  960\n",
      "cost:  8.338258\n",
      "=======================\n",
      "step:  980\n",
      "cost:  8.202539\n",
      "=======================\n",
      "step:  1000\n",
      "cost:  8.073024\n",
      "=======================\n",
      "step:  1020\n",
      "cost:  7.9495277\n",
      "=======================\n",
      "step:  1040\n",
      "cost:  7.83187\n",
      "=======================\n",
      "step:  1060\n",
      "cost:  7.7199025\n",
      "=======================\n",
      "step:  1080\n",
      "cost:  7.6134205\n",
      "=======================\n",
      "step:  1100\n",
      "cost:  7.512253\n",
      "=======================\n",
      "step:  1120\n",
      "cost:  7.416195\n",
      "=======================\n",
      "step:  1140\n",
      "cost:  7.3250847\n",
      "=======================\n",
      "step:  1160\n",
      "cost:  7.238747\n",
      "=======================\n",
      "step:  1180\n",
      "cost:  7.1569867\n",
      "=======================\n",
      "step:  1200\n",
      "cost:  7.079619\n",
      "=======================\n",
      "step:  1220\n",
      "cost:  7.006487\n",
      "=======================\n",
      "step:  1240\n",
      "cost:  6.937375\n",
      "=======================\n",
      "step:  1260\n",
      "cost:  6.8721538\n",
      "=======================\n",
      "step:  1280\n",
      "cost:  6.810629\n",
      "=======================\n",
      "step:  1300\n",
      "cost:  6.752636\n",
      "=======================\n",
      "step:  1320\n",
      "cost:  6.698017\n",
      "=======================\n",
      "step:  1340\n",
      "cost:  6.646622\n",
      "=======================\n",
      "step:  1360\n",
      "cost:  6.598288\n",
      "=======================\n",
      "step:  1380\n",
      "cost:  6.5528426\n",
      "=======================\n",
      "step:  1400\n",
      "cost:  6.5101647\n",
      "=======================\n",
      "step:  1420\n",
      "cost:  6.470113\n",
      "=======================\n",
      "step:  1440\n",
      "cost:  6.4325194\n",
      "=======================\n",
      "step:  1460\n",
      "cost:  6.3973036\n",
      "=======================\n",
      "step:  1480\n",
      "cost:  6.3642735\n",
      "=======================\n",
      "step:  1500\n",
      "cost:  6.3333745\n",
      "=======================\n",
      "step:  1520\n",
      "cost:  6.304428\n",
      "=======================\n",
      "step:  1540\n",
      "cost:  6.2773423\n",
      "=======================\n",
      "step:  1560\n",
      "cost:  6.2520337\n",
      "=======================\n",
      "step:  1580\n",
      "cost:  6.228368\n",
      "=======================\n",
      "step:  1600\n",
      "cost:  6.20625\n",
      "=======================\n",
      "step:  1620\n",
      "cost:  6.1855755\n",
      "=======================\n",
      "step:  1640\n",
      "cost:  6.1662555\n",
      "=======================\n",
      "step:  1660\n",
      "cost:  6.148217\n",
      "=======================\n",
      "step:  1680\n",
      "cost:  6.1313696\n",
      "=======================\n",
      "step:  1700\n",
      "cost:  6.115623\n",
      "=======================\n",
      "step:  1720\n",
      "cost:  6.1009374\n",
      "=======================\n",
      "step:  1740\n",
      "cost:  6.0871973\n",
      "=======================\n",
      "step:  1760\n",
      "cost:  6.07437\n",
      "=======================\n",
      "step:  1780\n",
      "cost:  6.062362\n",
      "=======================\n",
      "step:  1800\n",
      "cost:  6.051143\n",
      "=======================\n",
      "step:  1820\n",
      "cost:  6.040647\n",
      "=======================\n",
      "step:  1840\n",
      "cost:  6.030817\n",
      "=======================\n",
      "step:  1860\n",
      "cost:  6.0215926\n",
      "=======================\n",
      "step:  1880\n",
      "cost:  6.0129266\n",
      "=======================\n",
      "step:  1900\n",
      "cost:  6.004813\n",
      "=======================\n",
      "step:  1920\n",
      "cost:  5.9971795\n",
      "=======================\n",
      "step:  1940\n",
      "cost:  5.989989\n",
      "=======================\n",
      "step:  1960\n",
      "cost:  5.9832134\n",
      "=======================\n",
      "step:  1980\n",
      "cost:  5.976803\n",
      "=======================\n",
      "step:  2000\n",
      "cost:  5.97077\n",
      "=======================\n",
      "Your score will be  tf.Tensor([[187.30919]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "W = tf.Variable(tf.random.normal([3,1]))\n",
    "b = tf.Variable(tf.random.normal([1]))\n",
    "\n",
    "def cost():\n",
    "    hypothesis = tf.matmul(x_data, W) + b\n",
    "    cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
    "    return cost\n",
    "\n",
    "opt = tf.optimizers.Adam(lr=0.1)\n",
    "for step in range(2001):\n",
    "    hypo = tf.matmul(x_data, W) + b\n",
    "    opt.minimize(cost, var_list=[W,b])\n",
    "    if step % 20 == 0:\n",
    "        print(\"step: \",step)\n",
    "        print(\"cost: \",cost().numpy()\n",
    "        print(\"=======================\")\n",
    "\n",
    "print(\"Your score will be \", tf.matmul([[100.,70.,101.]], W)+ b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치 관련 더 고민해보자.\n",
    "\n",
    "\n",
    "xy = np.loadtxt('data-01-test-score.csv', delimiter=',', dtype=np.float32)\n",
    "x_data = xy[:,0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "W = tf.Variable(tf.random.normal([3,1]))\n",
    "b = tf.Variable(tf.random.normal([1]))\n",
    "\n",
    "def cost():\n",
    "    hypothesis = tf.matmul(x_data, W) + b\n",
    "    cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
    "    return cost\n",
    "\n",
    "\n",
    "\n",
    "opt = tf.optimizers.Adam(lr=0.1)\n",
    "for step in range(2001):\n",
    "    hypo = tf.matmul(x_data, W) + b\n",
    "    x_batch, y_batch = \n",
    "    opt.minimize(cost, var_list=[W,b])\n",
    "    if step % 20 == 0:\n",
    "        print(\"step: \",step)\n",
    "        print(\"cost: \",cost().numpy())\n",
    "        print(\"=======================\")\n",
    "\n",
    "print(\"Your score will be \", tf.matmul([[100.,70.,101.]], W)+ b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
